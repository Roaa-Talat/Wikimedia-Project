{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Team Members**\n",
        "\n",
        "\n",
        "| Name                 | ID            |\n",
        "|--------------------- |---------------|\n",
        "| Salma Mamdoh Sabry   | 20210162      |\n",
        "| Roaa Talat Mohamed   | 20210138      |\n",
        "| Shawky Ebrahim Ahmed | 20210184      |\n",
        "| Belal Ahmed Eid      | 20210092      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTptjIaPsFyJ"
      },
      "source": [
        "üìù **Assignment Introduction**\n",
        "------------------------------\n",
        "\n",
        "### üìò **Dataset Background**\n",
        "\n",
        "The dataset comes from the **Wikimedia Foundation**, which runs Wikipedia and other open-knowledge platforms. It contains **page view statistics** collected from **0:00 to 1:00 AM on January 1st, 2016**.\n",
        "\n",
        "Each line in the file represents the number of views for a specific page in that hour.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ **Schema**\n",
        "\n",
        "Each line has 4 fields separated by whitespace:\n",
        "\n",
        "| Field         | Description                                               |\n",
        "|---------------|-----------------------------------------------------------|\n",
        "| `Project Code`| Project identifier (e.g. `en` for English Wikipedia)      |\n",
        "| `Page Title`  | Title of the accessed page (e.g. `Political_status_of_Crimea`) |\n",
        "| `Page Hits`   | Number of times this page was accessed in that hour       |\n",
        "| `Page Size`   | Size of the page (likely in bytes)                        |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yE35O0BmXr9e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import LongType\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "import timeit\n",
        "import timeit\n",
        "from pyspark.sql import functions as F\n",
        "from itertools import combinations\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WikimediaPageViews\") \\\n",
        "    .master(\"local[*]\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Loading & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total lines loaded: 3,324,129\n"
          ]
        }
      ],
      "source": [
        "data_path = \"data.out\" \n",
        "raw_data = sc.textFile(data_path)\n",
        "print(f\"Total lines loaded: {raw_data.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Quality Report ===\n",
            "Total lines: 3,324,129\n",
            "Empty lines: 0\n",
            "Malformed lines: 103\n",
            "Valid lines: 3,324,026\n",
            "\n",
            "Sample malformed lines:\n",
            "ak.v  2 3606\n",
            "ar  526 21232283\n",
            "ar.s  4 38267\n",
            "ay.v  2 3606\n",
            "az  1 19081\n"
          ]
        }
      ],
      "source": [
        "def check_data_quality(rdd):\n",
        "    empty_lines = rdd.filter(lambda x: len(x.strip()) == 0).count()\n",
        "    malformed_lines = rdd.filter(lambda x: len(x.strip().split()) != 4).count()\n",
        "    \n",
        "    print(\"=== Data Quality Report ===\")\n",
        "    print(f\"Total lines: {rdd.count():,}\")\n",
        "    print(f\"Empty lines: {empty_lines:,}\")\n",
        "    print(f\"Malformed lines: {malformed_lines:,}\")\n",
        "    print(f\"Valid lines: {rdd.count() - empty_lines - malformed_lines:,}\")\n",
        "    \n",
        "    if malformed_lines > 0:\n",
        "        print(\"\\nSample malformed lines:\")\n",
        "        for line in rdd.filter(lambda x: len(x.strip().split()) != 4).take(5):\n",
        "            print(line)\n",
        "\n",
        "check_data_quality(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original count: 3,324,129\n",
            "Valid lines count: 3,324,026\n",
            "Removed 103 malformed lines\n",
            "\n",
            "Schema: (project_code: str, page_title: str, page_hits: int, page_size: int)\n",
            "\n",
            "('aa', '271_a.C', 1, 4675)\n",
            "('aa', 'Category:User_th', 1, 4770)\n",
            "('aa', 'Chiron_Elias_Krase', 1, 4694)\n",
            "('aa', 'Dassault_rafaele', 2, 9372)\n",
            "('aa', 'E.Desv', 1, 4662)\n"
          ]
        }
      ],
      "source": [
        "def parse_and_validate(line):\n",
        "    \"\"\"Parse line and validate it has 4 parts with the correct types\"\"\"\n",
        "    parts = line.strip().split()\n",
        "    if len(parts) != 4:\n",
        "        return None\n",
        "    try:\n",
        "        project_code = parts[0]\n",
        "        page_title = parts[1]\n",
        "        page_hits = int(parts[2])\n",
        "        page_size = int(parts[3])\n",
        "        return (project_code, page_title, page_hits, page_size)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "valid_lines = raw_data.filter(lambda line: parse_and_validate(line) is not None).cache()\n",
        "\n",
        "# Parsed and structured RDD from valid_lines\n",
        "parsed_rdd = valid_lines.map(parse_and_validate)\n",
        "\n",
        "# Count and display basic info\n",
        "total_count = raw_data.count()\n",
        "valid_count = valid_lines.count()\n",
        "print(f\"Original count: {total_count:,}\")\n",
        "print(f\"Valid lines count: {valid_count:,}\")\n",
        "print(f\"Removed {total_count - valid_count:,} malformed lines\")\n",
        "\n",
        "# Simulated schema and sample data\n",
        "print(\"\\nSchema: (project_code: str, page_title: str, page_hits: int, page_size: int)\\n\")\n",
        "for row in parsed_rdd.take(5):\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Page Size Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach ===\n",
            "Min size: 0 bytes\n",
            "Max size: 141,180,155,987 bytes\n",
            "Avg size: 132,215.80 bytes\n",
            "Execution time: 15.6108 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach ===\")\n",
        "\n",
        "def map_reduce_stats():\n",
        "    sizes = valid_lines.map(lambda line: int(line.split()[3]))\n",
        "    count = sizes.count()\n",
        "    total = sizes.reduce(lambda a, b: a + b)\n",
        "    min_size = sizes.reduce(lambda a, b: a if a < b else b)\n",
        "    max_size = sizes.reduce(lambda a, b: a if a > b else b)\n",
        "    \n",
        "    return {\n",
        "        'min': min_size,\n",
        "        'max': max_size,\n",
        "        'avg': total / count\n",
        "    }\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "mr_stats = map_reduce_stats()\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(f\"Min size: {mr_stats['min']:,} bytes\")\n",
        "print(f\"Max size: {mr_stats['max']:,} bytes\")\n",
        "print(f\"Avg size: {mr_stats['avg']:,.2f} bytes\")\n",
        "print(f\"Execution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Spark Loops Approach ===\n",
            "Min size: 0 bytes\n",
            "Max size: 141,180,155,987 bytes\n",
            "Avg size: 132,215.80 bytes\n",
            "Execution time: 2.8708 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Spark Loops Approach ===\")\n",
        "\n",
        "\n",
        "def spark_loops_stats():\n",
        "    sizes_list = []\n",
        "\n",
        "    for line in valid_lines.collect():  \n",
        "            size = int(line.split()[3])\n",
        "            sizes_list.append(size)\n",
        "\n",
        "    if not sizes_list:\n",
        "        return {'min': 0, 'max': 0, 'avg': 0}\n",
        "\n",
        "    count = len(sizes_list)\n",
        "    total = 0\n",
        "    min_size = sizes_list[0]\n",
        "    max_size = sizes_list[0]\n",
        "\n",
        "    for size in sizes_list:\n",
        "        total += size\n",
        "        if size < min_size:\n",
        "            min_size = size\n",
        "        if size > max_size:\n",
        "            max_size = size\n",
        "\n",
        "    return {\n",
        "        'min': min_size,\n",
        "        'max': max_size,\n",
        "        'avg': total / count\n",
        "    }\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "loop_stats = spark_loops_stats()\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "\n",
        "print(f\"Min size: {loop_stats['min']:,} bytes\")\n",
        "print(f\"Max size: {loop_stats['max']:,} bytes\")\n",
        "print(f\"Avg size: {loop_stats['avg']:,.2f} bytes\")\n",
        "print(f\"Execution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Comparison ===\n",
            "Map-Reduce Time: 15.6108 sec\n",
            "Spark Loops Time: 2.8708 sec\n",
            "Difference: 12.7400 sec\n",
            "Faster by: 5.44x\n",
            "+-----------+---+----------------+------------------+------------------+\n",
            "|   Approach|Min|             Max|               Avg|          Time_sec|\n",
            "+-----------+---+----------------+------------------+------------------+\n",
            "| Map-Reduce|0.0|1.41180155987E11|132215.79814237313|15.610841399990022|\n",
            "|Spark Loops|0.0|1.41180155987E11|132215.79814237313|2.8708305999898585|\n",
            "+-----------+---+----------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison \n",
        "print(\"\\n=== Performance Comparison ===\")\n",
        "print(f\"Map-Reduce Time: {mr_time:.4f} sec\")\n",
        "print(f\"Spark Loops Time: {loop_time:.4f} sec\")\n",
        "print(f\"Difference: {abs(mr_time - loop_time):.4f} sec\")\n",
        "print(f\"Faster by: {max(mr_time, loop_time)/min(mr_time, loop_time):.2f}x\")\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Approach\", StringType(), True),\n",
        "    StructField(\"Min\", DoubleType(), True),\n",
        "    StructField(\"Max\", DoubleType(), True),\n",
        "    StructField(\"Avg\", DoubleType(), True),\n",
        "    StructField(\"Time_sec\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (\"Map-Reduce\", float(mr_stats['min']), float(mr_stats['max']), float(mr_stats['avg']), float(mr_time)),\n",
        "    (\"Spark Loops\", float(loop_stats['min']), float(loop_stats['max']), float(loop_stats['avg']), float(loop_time))\n",
        "]\n",
        "\n",
        "results_df = spark.createDataFrame(data, schema)\n",
        "results_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚úÖ Observations:\n",
        "- Both approaches produced the **same statistics** in terms of `Min`, `Max`, and `Avg` page size.\n",
        "- However, the **Spark Loops approach outperformed the traditional Map-Reduce**, completing the task in just a third of the time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Count `The...` Titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach: Determine the number of page titles that start with the article ‚ÄúThe‚Äù. How many of those page titles are not part of the English project  ===\n",
            "Map-Reduce Approach:\n",
            "Titles starting with The:  45020\n",
            "Non-English titles starting with The:  10292\n",
            "\n",
            "Execution time: 8.4738 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach: Determine the number of page titles that start with the article ‚ÄúThe‚Äù. How many of those page titles are not part of the English project  ===\")\n",
        "\n",
        "def map_reduce_count_the_titles(data):\n",
        "    parsed_lines = data.map(lambda line: line.split())\n",
        "    the_titles = parsed_lines.filter(lambda record: record[1].startswith('The'))\n",
        "    the_titles_count = the_titles.count()\n",
        "    non_en_the = the_titles.filter(lambda record: record[0] != 'en')\n",
        "    non_en_the_count = non_en_the.count()\n",
        "    return the_titles_count, non_en_the_count\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "the_titles_count, non_en_the_count = map_reduce_count_the_titles(valid_lines)\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(\"Map-Reduce Approach:\")\n",
        "print(\"Titles starting with The: \", the_titles_count)\n",
        "print(\"Non-English titles starting with The: \", non_en_the_count)\n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Spark-loops: Determine the number of page titles that start with the article ‚ÄúThe‚Äù. How many of those page titles are not part of the English project  ===\n",
            "\n",
            "Spark Loops:\n",
            "Titles starting with The:  45020\n",
            "Non-English titles starting with The:  10292\n",
            "\n",
            "Execution time: 2.1718 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Spark-loops: Determine the number of page titles that start with the article ‚ÄúThe‚Äù. How many of those page titles are not part of the English project  ===\")\n",
        "\n",
        "def spark_loops_count_the_titles(data):\n",
        "    collected_data = data.collect()\n",
        "    the_titles_loops = 0\n",
        "    non_en_the_loops = 0\n",
        "\n",
        "    for line in collected_data:\n",
        "        record = line.split()\n",
        "        if record[1].startswith('The'):\n",
        "            the_titles_loops += 1\n",
        "            if record[0] != \"en\":\n",
        "                non_en_the_loops += 1\n",
        "    return the_titles_loops, non_en_the_loops\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "the_titles_loops, non_en_the_loops = spark_loops_count_the_titles(valid_lines)\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(\"\\nSpark Loops:\")\n",
        "print(\"Titles starting with The: \", the_titles_loops)\n",
        "print(\"Non-English titles starting with The: \", non_en_the_loops)\n",
        "print(f\"\\nExecution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Comparison ===\n",
            "Map-Reduce Time: 8.4738 sec\n",
            "Spark Loops Time: 2.1718 sec\n",
            "Difference: 6.3020 sec\n",
            "Faster by: 3.90x\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison \n",
        "print(\"\\n=== Performance Comparison ===\")\n",
        "print(f\"Map-Reduce Time: {mr_time:.4f} sec\")\n",
        "print(f\"Spark Loops Time: {loop_time:.4f} sec\")\n",
        "print(f\"Difference: {abs(mr_time - loop_time):.4f} sec\")\n",
        "print(f\"Faster by: {max(mr_time, loop_time)/min(mr_time, loop_time):.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚úÖObservations:\n",
        "- The **Spark Loop approach** outperforms the **Map-Reduce approach** in terms of execution time by a significant margin (3.5x faster).\n",
        "- Both approaches produce identical results, confirming that the logic and data are consistent across both methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Unique Title Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\E.J.S\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach: Determine the number of unique terms appearing in the page titles ===\n",
            "Total number of unique terms: 850479\n",
            "\n",
            "Execution time: 11.8674 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach: Determine the number of unique terms appearing in the page titles ===\")\n",
        "\n",
        "def map_reduce_unqiue_title_terms():\n",
        "    terms = valid_lines.flatMap(lambda line: line.split()[1].split(\"_\"))\n",
        "    normalized_terms = terms.map(lambda term: term.lower())\\\n",
        "                            .filter(lambda term: term.isalnum())\\\n",
        "                            .filter(lambda term: term not in stop_words)\n",
        "    unique_terms = normalized_terms.distinct()\n",
        "    return unique_terms\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "unique_terms = map_reduce_unqiue_title_terms()\n",
        "unique_terms_count = unique_terms.count()\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(f\"Total number of unique terms: {unique_terms_count}\")\n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loop-Based Approach Matching Map-Reduce Output ===\n",
            "Total number of unique terms: 850479\n",
            "\n",
            "Execution time: 4.5771 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Loop-Based Approach Matching Map-Reduce Output ===\")\n",
        "\n",
        "def spark_loops_unqiue_title_terms():\n",
        "    unique_terms = set()\n",
        "    for line in valid_lines.collect():\n",
        "        terms = line.strip().split()[1].split(\"_\")\n",
        "        for term in terms:\n",
        "            term = term.lower().strip()\n",
        "            if term.isalnum() and term not in stop_words:\n",
        "                unique_terms.add(term)\n",
        "    return unique_terms\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "unique_terms = spark_loops_unqiue_title_terms()\n",
        "unique_terms_count = len(unique_terms)\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(f\"Total number of unique terms: {unique_terms_count}\")\n",
        "print(f\"\\nExecution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Comparison ===\n",
            "Map-Reduce Time: 11.8674 sec\n",
            "Spark Loops Time: 4.5771 sec\n",
            "Difference: 7.2904 sec\n",
            "Faster by: 2.59x\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison \n",
        "print(\"\\n=== Performance Comparison ===\")\n",
        "print(f\"Map-Reduce Time: {mr_time:.4f} sec\")\n",
        "print(f\"Spark Loops Time: {loop_time:.4f} sec\")\n",
        "print(f\"Difference: {abs(mr_time - loop_time):.4f} sec\")\n",
        "print(f\"Faster by: {max(mr_time, loop_time)/min(mr_time, loop_time):.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚úÖObservations:\n",
        "\n",
        "- The **Spark Loop approach** outperforms the **Map-Reduce approach** in terms of execution time by a significant margin (3.4x faster).\n",
        "- Both approaches produce identical results, confirming that the logic and data are consistent across both methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Title Count Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach for Title Counts ===\n",
            "Total unique titles: 2,968,690\n",
            "\n",
            "Top 20 titles by count:\n",
            " 1. water                                                   118\n",
            " 2. 1863                                                    106\n",
            " 3. Berlin                                                  101\n",
            " 4. Google                                                  101\n",
            " 5. Linux                                                    98\n",
            " 6. Main_Page                                                90\n",
            " 7. ISO_3166-1                                               88\n",
            " 8. Microsoft_Windows                                        87\n",
            " 9. HTML                                                     86\n",
            "10. Index.php                                                86\n",
            "11. Frank_Lloyd_Wright                                       85\n",
            "12. PHP                                                      83\n",
            "13. ISO_4217                                                 76\n",
            "14. Boston                                                   75\n",
            "15. Special:Search                                           74\n",
            "16. Wikimedia_Commons                                        70\n",
            "17. Pennsylvania                                             69\n",
            "18. Sir_Francis_Seymour_Haden                                68\n",
            "19. Lorens_Frolich                                           68\n",
            "20. Andreas_Schimpf                                          68\n",
            "\n",
            "Execution time: 25.6639 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach for Title Counts ===\")\n",
        "\n",
        "def map_reduce_title_counts():\n",
        "    title_ones = valid_lines.map(lambda line: (line.split()[1], 1))\n",
        "    title_counts = title_ones.reduceByKey(lambda a, b: a + b)\n",
        "    \n",
        "    sorted_titles_rdd = title_counts.map(lambda x: (x[1], x[0])) \\\n",
        "                                    .sortByKey(ascending=False) \\\n",
        "                                    .map(lambda x: (x[1], x[0]))  # Back to (title, count)\n",
        "    \n",
        "    total_unique = title_counts.count()  \n",
        "    top_20_titles = sorted_titles_rdd.take(20) \n",
        "\n",
        "    return {\n",
        "        'total_unique': total_unique,\n",
        "        'top_titles': top_20_titles,\n",
        "        'sorted_titles_rdd': sorted_titles_rdd  \n",
        "    }\n",
        "\n",
        "# Time execution\n",
        "start_time = timeit.default_timer()\n",
        "title_stats = map_reduce_title_counts()\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Total unique titles: {title_stats['total_unique']:,}\")\n",
        "print(\"\\nTop 20 titles by count:\")\n",
        "for idx, (title, count) in enumerate(title_stats['top_titles'], 1):\n",
        "    print(f\"{idx:2d}. {title[:50]:<50} {count:>8,}\")\n",
        "\n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique titles: 2,968,690\n",
            "\n",
            "Top 20 titles by count:\n",
            " 1. water                                                   118\n",
            " 2. 1863                                                    106\n",
            " 3. Berlin                                                  101\n",
            " 4. Google                                                  101\n",
            " 5. Linux                                                    98\n",
            " 6. Main_Page                                                90\n",
            " 7. ISO_3166-1                                               88\n",
            " 8. Microsoft_Windows                                        87\n",
            " 9. Index.php                                                86\n",
            "10. HTML                                                     86\n",
            "11. Frank_Lloyd_Wright                                       85\n",
            "12. PHP                                                      83\n",
            "13. ISO_4217                                                 76\n",
            "14. Boston                                                   75\n",
            "15. Special:Search                                           74\n",
            "16. Wikimedia_Commons                                        70\n",
            "17. Pennsylvania                                             69\n",
            "18. Andreas_Schimpf                                          68\n",
            "19. Harriet_Gouldsmith                                       68\n",
            "20. Lorens_Frolich                                           68\n",
            "\n",
            "Execution time: 3.3709 seconds\n"
          ]
        }
      ],
      "source": [
        "def spark_loops_title_counts():\n",
        "    all_lines = valid_lines.collect()  \n",
        "\n",
        "    title_counts = {}\n",
        "\n",
        "    for line in all_lines:\n",
        "        title = line.split()[1]  \n",
        "        title_counts[title] = title_counts.get(title, 0) + 1  \n",
        "\n",
        "    sorted_titles = sorted(title_counts.items(), key=lambda x: -x[1])\n",
        "\n",
        "    return {\n",
        "        'total_unique': len(sorted_titles),\n",
        "        'top_titles': sorted_titles[:20],  \n",
        "        'all_counts': sorted_titles        \n",
        "    }\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "title_stats_loop = spark_loops_title_counts()\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Total unique titles: {title_stats_loop['total_unique']:,}\")\n",
        "print(\"\\nTop 20 titles by count:\")\n",
        "for idx, (title, count) in enumerate(title_stats_loop['top_titles'], 1):\n",
        "    print(f\"{idx:2d}. {title[:50]:<50} {count:>8,}\")\n",
        "\n",
        "print(f\"\\nExecution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Comparison for Title Counts ===\n",
            "Map-Reduce Time: 25.6639 sec\n",
            "Python Loop Time: 3.3709 sec\n",
            "Difference: 22.2930 sec\n",
            "Python Loop was 7.6x faster\n",
            "\n",
            "Performance Comparison Results:\n",
            "+-----------+-------------+---------+-----------------+\n",
            "|Approach   |Unique_Titles|Top_Count|Time_sec         |\n",
            "+-----------+-------------+---------+-----------------+\n",
            "|Map-Reduce |2968690      |118      |25.66392029999406|\n",
            "|Python Loop|2968690      |118      |3.37089219999325 |\n",
            "+-----------+-------------+---------+-----------------+\n",
            "\n",
            "\n",
            "=== Verification ===\n",
            "Unique counts match: True\n",
            "Top count match: True\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison for Title Counts \n",
        "print(\"\\n=== Performance Comparison for Title Counts ===\")\n",
        "print(f\"Map-Reduce Time: {mr_time:.4f} sec\")\n",
        "print(f\"Python Loop Time: {loop_time:.4f} sec\")\n",
        "print(f\"Difference: {abs(mr_time - loop_time):.4f} sec\")\n",
        "print(f\"Python Loop was {mr_time/loop_time:.1f}x faster\")\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Approach\", StringType(), True),\n",
        "    StructField(\"Unique_Titles\", LongType(), True),\n",
        "    StructField(\"Top_Count\", LongType(), True),\n",
        "    StructField(\"Time_sec\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "top_count_mr = title_stats['top_titles'][0][1] if title_stats['top_titles'] else 0\n",
        "top_count_loop = title_stats_loop['top_titles'][0][1] if title_stats_loop['top_titles'] else 0\n",
        "\n",
        "data = [\n",
        "    Row(\"Map-Reduce\", title_stats['total_unique'], top_count_mr, float(mr_time)),\n",
        "    Row(\"Python Loop\", title_stats_loop['total_unique'], top_count_loop, float(loop_time))\n",
        "]\n",
        "\n",
        "title_comparison_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(\"\\nPerformance Comparison Results:\")\n",
        "title_comparison_df.show(truncate=False)\n",
        "\n",
        "print(\"\\n=== Verification ===\")\n",
        "print(f\"Unique counts match: {title_stats['total_unique'] == title_stats_loop['total_unique']}\")\n",
        "print(f\"Top count match: {top_count_mr == top_count_loop}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖObservations:\n",
        "- The **Python Loop approach** outperforms the **Map-Reduce approach** in terms of execution time by a significant margin (3.0x faster).\n",
        "- Both approaches produce identical results for **Unique Titles** and **Top Count**, confirming that the logic and data are consistent across both methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Grouping by Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach: Combine data of pages with the same title ===\n",
            "\n",
            "Sample output (first 5 grouped titles):\n",
            "\n",
            "Title: Indonesian_Wikipedia\n",
            "  - Project: aa, Hits: 1, Size: 4679\n",
            "  - Project: en, Hits: 1, Size: 93905\n",
            "\n",
            "Title: Special:MyLanguage/Meta:Index\n",
            "  - Project: aa, Hits: 1, Size: 4701\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  - Project: aa, Hits: 1, Size: 5556\n",
            "  - Project: commons.m, Hits: 2, Size: 15231\n",
            "  - Project: en, Hits: 5, Size: 101406\n",
            "  - Project: en.s, Hits: 1, Size: 8597\n",
            "  - Project: en.voy, Hits: 1, Size: 8550\n",
            "  - Project: meta.m, Hits: 1, Size: 11529\n",
            "  - Project: outreach.m, Hits: 1, Size: 5698\n",
            "  - Project: simple, Hits: 3, Size: 32145\n",
            "\n",
            "Title: Special:WhatLinksHere/MediaWiki:Edittools\n",
            "  - Project: aa, Hits: 1, Size: 5139\n",
            "\n",
            "Title: User:IlStudioso\n",
            "  - Project: aa, Hits: 1, Size: 6796\n",
            "\n",
            "\n",
            "Execution time: 11.2713 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach: Combine data of pages with the same title ===\")\n",
        "\n",
        "def map_reduce_combine_all_by_title():\n",
        "    title_data_rdd = valid_lines.map(lambda line: line.split()) \\\n",
        "                                .filter(lambda parts: len(parts) == 4) \\\n",
        "                                .map(lambda parts: (parts[1], (parts[0], int(parts[2]), int(parts[3]))))\n",
        "    \n",
        "    grouped_by_title = title_data_rdd.groupByKey()\n",
        "    \n",
        "    return grouped_by_title\n",
        "\n",
        "# Time execution\n",
        "start_time = timeit.default_timer()\n",
        "combined_rdd = map_reduce_combine_all_by_title()\n",
        "sample_combined = combined_rdd.take(5)\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Show results\n",
        "print(\"\\nSample output (first 5 grouped titles):\\n\")\n",
        "for title, page_data_list in sample_combined:\n",
        "    print(f\"Title: {title}\")\n",
        "    for project_code, hits, size in page_data_list:\n",
        "        print(f\"  - Project: {project_code}, Hits: {hits}, Size: {size}\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach: Aggregate data of pages with the same title ===\n",
            "\n",
            "Sample output (first 5 aggregated titles):\n",
            "\n",
            "Title: Indonesian_Wikipedia\n",
            "  - Projects: ['aa', 'en']\n",
            "  - Total Hits: 2\n",
            "  - Total Size: 98584\n",
            "\n",
            "Title: Special:MyLanguage/Meta:Index\n",
            "  - Projects: ['aa']\n",
            "  - Total Hits: 1\n",
            "  - Total Size: 4701\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  - Projects: ['aa', 'commons.m', 'en', 'en.s', 'en.voy', 'meta.m', 'outreach.m', 'simple']\n",
            "  - Total Hits: 15\n",
            "  - Total Size: 188712\n",
            "\n",
            "Title: Special:WhatLinksHere/MediaWiki:Edittools\n",
            "  - Projects: ['aa']\n",
            "  - Total Hits: 1\n",
            "  - Total Size: 5139\n",
            "\n",
            "Title: User:IlStudioso\n",
            "  - Projects: ['aa']\n",
            "  - Total Hits: 1\n",
            "  - Total Size: 6796\n",
            "\n",
            "\n",
            "Execution time: 12.1982 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach: Aggregate data of pages with the same title ===\")\n",
        "\n",
        "def map_reduce_aggregate_by_title():\n",
        "    title_data_rdd = valid_lines.map(lambda line: line.split()) \\\n",
        "                                .filter(lambda parts: len(parts) == 4) \\\n",
        "                                .map(lambda parts: (parts[1], (parts[0], int(parts[2]), int(parts[3]))))\n",
        "    \n",
        "    aggregated_rdd = title_data_rdd.mapValues(lambda x: ([x[0]], x[1], x[2])) \\\n",
        "                                   .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
        "\n",
        "    return aggregated_rdd\n",
        "\n",
        "# Time execution\n",
        "start_time = timeit.default_timer()\n",
        "aggregated_rdd = map_reduce_aggregate_by_title()\n",
        "sample_aggregated = aggregated_rdd.take(5)\n",
        "agg_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Show results\n",
        "print(\"\\nSample output (first 5 aggregated titles):\\n\")\n",
        "for title, (project_codes, total_hits, total_size) in sample_aggregated:\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"  - Projects: {project_codes}\")\n",
        "    print(f\"  - Total Hits: {total_hits}\")\n",
        "    print(f\"  - Total Size: {total_size}\\n\")\n",
        "\n",
        "print(f\"\\nExecution time: {agg_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loop-Based Approach: Aggregate data of pages with the same title ===\n",
            "\n",
            "Sample output (first 5 aggregated titles):\n",
            "\n",
            "Title: 271_a.C\n",
            "  - Project Codes: ['aa', 'az', 'bcl', 'be']\n",
            "  - Total Hits: 4\n",
            "  - Total Size: 22386\n",
            "\n",
            "Title: Category:User_th\n",
            "  - Project Codes: ['aa', 'commons.m']\n",
            "  - Total Hits: 2\n",
            "  - Total Size: 4770\n",
            "\n",
            "Title: Chiron_Elias_Krase\n",
            "  - Project Codes: ['aa', 'az', 'bg', 'cho', 'dz', 'it']\n",
            "  - Total Hits: 6\n",
            "  - Total Size: 34584\n",
            "\n",
            "Title: Dassault_rafaele\n",
            "  - Project Codes: ['aa', 'en', 'it']\n",
            "  - Total Hits: 4\n",
            "  - Total Size: 21940\n",
            "\n",
            "Title: E.Desv\n",
            "  - Project Codes: ['aa', 'arc', 'ast', 'fiu-vro', 'fr', 'ik']\n",
            "  - Total Hits: 6\n",
            "  - Total Size: 31539\n",
            "\n",
            "\n",
            "Execution time: 9.8272 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Loop-Based Approach: Aggregate data of pages with the same title ===\")\n",
        "\n",
        "def loop_based_aggregate_by_title():\n",
        "    collected_data = valid_lines.collect()\n",
        "    \n",
        "    title_dict = {}\n",
        "\n",
        "    for line in collected_data:\n",
        "        parts = line.split()\n",
        "        if len(parts) != 4:\n",
        "            continue  \n",
        "\n",
        "        project_code = parts[0]\n",
        "        title = parts[1]\n",
        "        hits = int(parts[2])\n",
        "        size = int(parts[3])\n",
        "\n",
        "        if title not in title_dict:\n",
        "            title_dict[title] = {\n",
        "                'project_codes': [project_code],\n",
        "                'total_hits': hits,\n",
        "                'total_size': size\n",
        "            }\n",
        "        else:\n",
        "            title_dict[title]['project_codes'].append(project_code)\n",
        "            title_dict[title]['total_hits'] += hits\n",
        "            title_dict[title]['total_size'] += size\n",
        "    \n",
        "    return title_dict\n",
        "\n",
        "# Time execution\n",
        "start_time = timeit.default_timer()\n",
        "aggregated_results = loop_based_aggregate_by_title()\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSample output (first 5 aggregated titles):\\n\")\n",
        "for i, (title, data) in enumerate(aggregated_results.items()):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"  - Project Codes: {data['project_codes']}\")\n",
        "    print(f\"  - Total Hits: {data['total_hits']}\")\n",
        "    print(f\"  - Total Size: {data['total_size']}\\n\")\n",
        "\n",
        "print(f\"\\nExecution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ Observations\n",
        "\n",
        "The **Map-Reduce approach** is significantly faster due to its parallel processing capabilities, which allows tasks to be distributed across multiple nodes, reducing execution time. In contrast, the **Loop-Based approach** processes data sequentially, which becomes slower as the dataset grows, leading to longer execution times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach: Generate Page Combinations Per Title ===\n",
            "\n",
            "Sample output (first 5 title-based page combinations):\n",
            "\n",
            "Title: Indonesian_Wikipedia\n",
            "  - Page 1 ‚Üí Project: aa, Hits: 1, Size: 4679\n",
            "  - Page 2 ‚Üí Project: en, Hits: 1, Size: 93905\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  - Page 1 ‚Üí Project: aa, Hits: 1, Size: 5556\n",
            "  - Page 2 ‚Üí Project: commons.m, Hits: 2, Size: 15231\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  - Page 1 ‚Üí Project: aa, Hits: 1, Size: 5556\n",
            "  - Page 2 ‚Üí Project: en, Hits: 5, Size: 101406\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  - Page 1 ‚Üí Project: aa, Hits: 1, Size: 5556\n",
            "  - Page 2 ‚Üí Project: en.s, Hits: 1, Size: 8597\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  - Page 1 ‚Üí Project: aa, Hits: 1, Size: 5556\n",
            "  - Page 2 ‚Üí Project: en.voy, Hits: 1, Size: 8550\n",
            "\n",
            "\n",
            "Execution time: 2.4744 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach: Generate Page Combinations Per Title ===\")\n",
        "\n",
        "def generate_page_combinations(combined_rdd):\n",
        "    # For each title, generate all unique pairs from its associated pages\n",
        "    title_combinations_rdd = combined_rdd.flatMapValues(\n",
        "        lambda pages: list(combinations(pages, 2))\n",
        "    )\n",
        "    return title_combinations_rdd\n",
        "\n",
        "# Time execution\n",
        "start_time = timeit.default_timer()\n",
        "title_page_combinations_rdd = generate_page_combinations(combined_rdd)\n",
        "sample_pairs = title_page_combinations_rdd.take(5)\n",
        "combo_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSample output (first 5 title-based page combinations):\\n\")\n",
        "for title, ((proj1, hits1, size1), (proj2, hits2, size2)) in sample_pairs:\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"  - Page 1 ‚Üí Project: {proj1}, Hits: {hits1}, Size: {size1}\")\n",
        "    print(f\"  - Page 2 ‚Üí Project: {proj2}, Hits: {hits2}, Size: {size2}\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nExecution time: {combo_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All results written to final_output_combined_for_the_same_title.out\n"
          ]
        }
      ],
      "source": [
        "all_combined = title_page_combinations_rdd.collect()\n",
        "\n",
        "# Write all results to a file\n",
        "output_path = \"final_output_combined_for_the_same_title.out\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== Combined Page Pairs grouprd by Titles ===\\n\")\n",
        "    for title, pairs in all_combined:\n",
        "        f.write(f\"Title: {title}\")\n",
        "        f.write(f\"  - Page 1 ‚Üí Project: {proj1}, Hits: {hits1}, Size: {size1}\")\n",
        "        f.write(f\"  - Page 2 ‚Üí Project: {proj2}, Hits: {hits2}, Size: {size2}\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"All results written to {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
