{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTptjIaPsFyJ"
      },
      "source": [
        "📝 **Assignment Introduction**\n",
        "------------------------------\n",
        "\n",
        "### 📘 **Dataset Background**\n",
        "\n",
        "The dataset comes from the **Wikimedia Foundation**, which runs Wikipedia and other open-knowledge platforms. It contains **page view statistics** collected from **0:00 to 1:00 AM on January 1st, 2016**.\n",
        "\n",
        "Each line in the file represents the number of views for a specific page in that hour.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔢 **Schema**\n",
        "\n",
        "Each line has 4 fields separated by whitespace:\n",
        "\n",
        "| Field         | Description                                               |\n",
        "|---------------|-----------------------------------------------------------|\n",
        "| `Project Code`| Project identifier (e.g. `en` for English Wikipedia)      |\n",
        "| `Page Title`  | Title of the accessed page (e.g. `Political_status_of_Crimea`) |\n",
        "| `Page Hits`   | Number of times this page was accessed in that hour       |\n",
        "| `Page Size`   | Size of the page (likely in bytes)                        |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE35O0BmXr9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Shawky\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import LongType\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "import timeit,os,time\n",
        "import shutil\n",
        "from operator import add\n",
        "import timeit\n",
        "from pyspark.sql import functions as F\n",
        "from itertools import combinations\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WikimediaPageViews\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"hadoop.home.dir\", \"C:\\\\hadoop\") \\\n",
        "    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
        "    .config(\"spark.hadoop.io.native.lib.available\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Loading & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total lines loaded: 3,324,129\n"
          ]
        }
      ],
      "source": [
        "data_path = \"data.out\" \n",
        "raw_data = sc.textFile(data_path)\n",
        "print(f\"Total lines loaded: {raw_data.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Quality Report ===\n",
            "Total lines: 3,324,129\n",
            "Empty lines: 0\n",
            "Malformed lines: 103\n",
            "Valid lines: 3,324,026\n",
            "\n",
            "Sample malformed lines:\n",
            "ak.v  2 3606\n",
            "ar  526 21232283\n",
            "ar.s  4 38267\n",
            "ay.v  2 3606\n",
            "az  1 19081\n"
          ]
        }
      ],
      "source": [
        "def check_data_quality(rdd):\n",
        "    empty_lines = rdd.filter(lambda x: len(x.strip()) == 0).count()\n",
        "    malformed_lines = rdd.filter(lambda x: len(x.strip().split()) != 4).count()\n",
        "    \n",
        "    print(\"=== Data Quality Report ===\")\n",
        "    print(f\"Total lines: {rdd.count():,}\")\n",
        "    print(f\"Empty lines: {empty_lines:,}\")\n",
        "    print(f\"Malformed lines: {malformed_lines:,}\")\n",
        "    print(f\"Valid lines: {rdd.count() - empty_lines - malformed_lines:,}\")\n",
        "    \n",
        "    if malformed_lines > 0:\n",
        "        print(\"\\nSample malformed lines:\")\n",
        "        for line in rdd.filter(lambda x: len(x.strip().split()) != 4).take(5):\n",
        "            print(line)\n",
        "\n",
        "check_data_quality(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original count: 3,324,129\n",
            "Valid lines count: 3,324,026\n",
            "Removed 103 malformed lines\n"
          ]
        }
      ],
      "source": [
        "def is_valid_line(line):\n",
        "    \"\"\"Check if line has exactly 4 fields with proper types\"\"\"\n",
        "    parts = line.strip().split()\n",
        "    if len(parts) != 4:\n",
        "        return False\n",
        "    try:\n",
        "        int(parts[2])  # Verify page_hits is numeric\n",
        "        int(parts[3])  # Verify page_size is numeric\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "valid_lines = raw_data.filter(is_valid_line).cache()\n",
        "print(f\"Original count: {raw_data.count():,}\")\n",
        "print(f\"Valid lines count: {valid_lines.count():,}\")\n",
        "print(f\"Removed {raw_data.count() - valid_lines.count():,} malformed lines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- project_code: string (nullable = true)\n",
            " |-- page_title: string (nullable = true)\n",
            " |-- page_hits: long (nullable = true)\n",
            " |-- page_size: long (nullable = true)\n",
            "\n",
            "+------------+------------------+---------+---------+\n",
            "|project_code|        page_title|page_hits|page_size|\n",
            "+------------+------------------+---------+---------+\n",
            "|          aa|           271_a.C|        1|     4675|\n",
            "|          aa|  Category:User_th|        1|     4770|\n",
            "|          aa|Chiron_Elias_Krase|        1|     4694|\n",
            "|          aa|  Dassault_rafaele|        2|     9372|\n",
            "|          aa|            E.Desv|        1|     4662|\n",
            "+------------+------------------+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_rdd = valid_lines.map(lambda line: line.split(\" \")) \\\n",
        "                .filter(lambda parts: len(parts) >= 4)\n",
        "\n",
        "structured_rdd = parsed_rdd.map(lambda parts: (\n",
        "    parts[0],                  # project_code\n",
        "    parts[1],                  # page_title\n",
        "    int(parts[2]),             # number_of_views\n",
        "    int(parts[3])              # bytes_transferred\n",
        "))\n",
        "\n",
        "df = structured_rdd.toDF([\"project_code\", \"page_title\", \"page_hits\", \"page_size\"])\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GJbV2013j_xX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data Validation:\n",
            "Rows with null values: 0\n",
            "Rows with negative values: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nData Validation:\")\n",
        "null_check = df.filter(\n",
        "    col(\"page_hits\").isNull() | \n",
        "    col(\"page_size\").isNull()\n",
        ").count()\n",
        "print(f\"Rows with null values: {null_check}\")\n",
        "\n",
        "negative_check = df.filter(\n",
        "    (col(\"page_hits\") < 0) | \n",
        "    (col(\"page_size\") < 0)\n",
        ").count()\n",
        "print(f\"Rows with negative values: {negative_check}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Basic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Basic Statistics ===\n",
            "Total projects: 1077\n",
            "Total pages: 3,324,026\n",
            "\n",
            "Top 10 most viewed pages:\n",
            "+------------+----------+---------+------------+\n",
            "|project_code|page_title|page_hits|page_size   |\n",
            "+------------+----------+---------+------------+\n",
            "|en.mw       |en        |5466346  |141180155987|\n",
            "|es.mw       |es        |695531   |12261337515 |\n",
            "|ja.mw       |ja        |611443   |15021588551 |\n",
            "|de.mw       |de        |572119   |9523069696  |\n",
            "|fr.mw       |fr        |536978   |11752030020 |\n",
            "|ru.mw       |ru        |466742   |11847816616 |\n",
            "|it.mw       |it        |400297   |8176042087  |\n",
            "|en          |Main_Page |257915   |4289970372  |\n",
            "|pt.mw       |pt        |196160   |4029404403  |\n",
            "|pl.mw       |pl        |176059   |2782453516  |\n",
            "+------------+----------+---------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Size statistics (bytes):\n",
            "+-------+-------------------+\n",
            "|summary|          page_size|\n",
            "+-------+-------------------+\n",
            "|  count|            3324026|\n",
            "|   mean| 132215.79814237313|\n",
            "| stddev|7.912571976826106E7|\n",
            "|    min|                  0|\n",
            "|    max|       141180155987|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Basic Statistics ===\")\n",
        "print(f\"Total projects: {df.select('project_code').distinct().count()}\")\n",
        "print(f\"Total pages: {df.count():,}\")\n",
        "\n",
        "print(\"\\nTop 10 most viewed pages:\")\n",
        "df.orderBy(col(\"page_hits\").desc()).show(10, truncate=False)\n",
        "\n",
        "print(\"\\nSize statistics (bytes):\")\n",
        "df.select(\"page_size\").describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Temporary view 'cleaned_wiki_data' created\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView(\"cleaned_wiki_data\")\n",
        "print(\"\\nTemporary view 'cleaned_wiki_data' created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cleared Spark cache\n"
          ]
        }
      ],
      "source": [
        "spark.catalog.clearCache()\n",
        "print(\"\\nCleared Spark cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Page Size Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach ===\n",
            "Min size: 0 bytes\n",
            "Max size: 141,180,155,987 bytes\n",
            "Avg size: 132,215.80 bytes\n",
            "Execution time: 13.7272 seconds\n"
          ]
        }
      ],
      "source": [
        "# Map-Reduce Approach for Page Size Stats\n",
        "print(\"=== Map-Reduce Approach ===\")\n",
        "\n",
        "def map_reduce_stats():\n",
        "    # Map phase: Extract sizes\n",
        "    sizes = valid_lines.map(lambda line: int(line.split()[3]))\n",
        "    \n",
        "    # Reduce phase: Calculate stats\n",
        "    count = sizes.count()\n",
        "    total = sizes.reduce(lambda a, b: a + b)\n",
        "    min_size = sizes.reduce(lambda a, b: a if a < b else b)\n",
        "    max_size = sizes.reduce(lambda a, b: a if a > b else b)\n",
        "    \n",
        "    return {\n",
        "        'min': min_size,\n",
        "        'max': max_size,\n",
        "        'avg': total / count\n",
        "    }\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "mr_stats = map_reduce_stats()\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(f\"Min size: {mr_stats['min']:,} bytes\")\n",
        "print(f\"Max size: {mr_stats['max']:,} bytes\")\n",
        "print(f\"Avg size: {mr_stats['avg']:,.2f} bytes\")\n",
        "print(f\"Execution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Spark Loops Approach ===\n",
            "Min size: 0 bytes\n",
            "Max size: 141,180,155,987 bytes\n",
            "Avg size: 132,215.80 bytes\n",
            "Execution time: 3.3671 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Spark Loops Approach ===\")\n",
        "\n",
        "\n",
        "def spark_loops_stats():\n",
        "    sizes_list = []\n",
        "\n",
        "    for line in valid_lines.collect():  # collect all lines into Python list\n",
        "            size = int(line.split()[3])\n",
        "            sizes_list.append(size)\n",
        "\n",
        "    if not sizes_list:\n",
        "        return {'min': 0, 'max': 0, 'avg': 0}\n",
        "\n",
        "    count = len(sizes_list)\n",
        "    total = 0\n",
        "    min_size = sizes_list[0]\n",
        "    max_size = sizes_list[0]\n",
        "\n",
        "    for size in sizes_list:\n",
        "        total += size\n",
        "        if size < min_size:\n",
        "            min_size = size\n",
        "        if size > max_size:\n",
        "            max_size = size\n",
        "\n",
        "    return {\n",
        "        'min': min_size,\n",
        "        'max': max_size,\n",
        "        'avg': total / count\n",
        "    }\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "loop_stats = spark_loops_stats()\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Print results in the same format\n",
        "print(f\"Min size: {loop_stats['min']:,} bytes\")\n",
        "print(f\"Max size: {loop_stats['max']:,} bytes\")\n",
        "print(f\"Avg size: {loop_stats['avg']:,.2f} bytes\")\n",
        "print(f\"Execution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Comparison ===\n",
            "Map-Reduce Time: 13.7272 sec\n",
            "Spark Loops Time: 3.3671 sec\n",
            "Difference: 10.3602 sec\n",
            "Faster by: 4.08x\n",
            "+-----------+---+----------------+------------------+------------------+\n",
            "|   Approach|Min|             Max|               Avg|          Time_sec|\n",
            "+-----------+---+----------------+------------------+------------------+\n",
            "| Map-Reduce|0.0|1.41180155987E11|132215.79814237313| 13.72724479995668|\n",
            "|Spark Loops|0.0|1.41180155987E11|132215.79814237313|3.3670592999551445|\n",
            "+-----------+---+----------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison \n",
        "print(\"\\n=== Performance Comparison ===\")\n",
        "print(f\"Map-Reduce Time: {mr_time:.4f} sec\")\n",
        "print(f\"Spark Loops Time: {loop_time:.4f} sec\")\n",
        "print(f\"Difference: {abs(mr_time - loop_time):.4f} sec\")\n",
        "print(f\"Faster by: {max(mr_time, loop_time)/min(mr_time, loop_time):.2f}x\")\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Approach\", StringType(), True),\n",
        "    StructField(\"Min\", DoubleType(), True),\n",
        "    StructField(\"Max\", DoubleType(), True),\n",
        "    StructField(\"Avg\", DoubleType(), True),\n",
        "    StructField(\"Time_sec\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (\"Map-Reduce\", float(mr_stats['min']), float(mr_stats['max']), float(mr_stats['avg']), float(mr_time)),\n",
        "    (\"Spark Loops\", float(loop_stats['min']), float(loop_stats['max']), float(loop_stats['avg']), float(loop_time))\n",
        "]\n",
        "\n",
        "results_df = spark.createDataFrame(data, schema)\n",
        "results_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ✅ Observations:\n",
        "- Both approaches produced the **same statistics** in terms of `Min`, `Max`, and `Avg` page size.\n",
        "- However, the **Spark Loops approach outperformed the traditional Map-Reduce**, completing the task in just a third of the time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Title Count Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach for Title Counts ===\n",
            "Total unique titles: 2,968,690\n",
            "\n",
            "Top 20 titles by count:\n",
            " 1. water                                                   118\n",
            " 2. 1863                                                    106\n",
            " 3. Berlin                                                  101\n",
            " 4. Google                                                  101\n",
            " 5. Linux                                                    98\n",
            " 6. Main_Page                                                90\n",
            " 7. ISO_3166-1                                               88\n",
            " 8. Microsoft_Windows                                        87\n",
            " 9. HTML                                                     86\n",
            "10. Index.php                                                86\n",
            "11. Frank_Lloyd_Wright                                       85\n",
            "12. PHP                                                      83\n",
            "13. ISO_4217                                                 76\n",
            "14. Boston                                                   75\n",
            "15. Special:Search                                           74\n",
            "16. Wikimedia_Commons                                        70\n",
            "17. Pennsylvania                                             69\n",
            "18. Sir_Francis_Seymour_Haden                                68\n",
            "19. Lorens_Frolich                                           68\n",
            "20. Andreas_Schimpf                                          68\n",
            "\n",
            "Execution time: 11.2421 seconds\n"
          ]
        }
      ],
      "source": [
        "# Map-Reduce Approach for Title Counts\n",
        "print(\"=== Map-Reduce Approach for Title Counts ===\")\n",
        "\n",
        "def map_reduce_title_counts():\n",
        "    # Map phase: Create (title, 1) pairs\n",
        "    title_ones = valid_lines.map(lambda line: (line.split()[1], 1))\n",
        "    \n",
        "    # Reduce phase: Sum counts by title\n",
        "    title_counts = title_ones.reduceByKey(lambda a, b: a + b)\n",
        "    \n",
        "    # Collect all results\n",
        "    all_titles = title_counts.collect()\n",
        "    \n",
        "    # Sort by count descending\n",
        "    sorted_titles = sorted(all_titles, key=lambda x: -x[1])\n",
        "    \n",
        "    return {\n",
        "        'total_unique': len(sorted_titles),\n",
        "        'top_titles': sorted_titles[:20],  # Top 20\n",
        "        'all_counts': sorted_titles        # All titles\n",
        "    }\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "title_stats = map_reduce_title_counts()\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Total unique titles: {title_stats['total_unique']:,}\")\n",
        "print(\"\\nTop 20 titles by count:\")\n",
        "for idx, (title, count) in enumerate(title_stats['top_titles'], 1):\n",
        "    print(f\"{idx:2d}. {title[:50]:<50} {count:>8,}\")\n",
        "\n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique titles: 2,968,690\n",
            "\n",
            "Top 20 titles by count:\n",
            " 1. water                                                   118\n",
            " 2. 1863                                                    106\n",
            " 3. Berlin                                                  101\n",
            " 4. Google                                                  101\n",
            " 5. Linux                                                    98\n",
            " 6. Main_Page                                                90\n",
            " 7. ISO_3166-1                                               88\n",
            " 8. Microsoft_Windows                                        87\n",
            " 9. Index.php                                                86\n",
            "10. HTML                                                     86\n",
            "11. Frank_Lloyd_Wright                                       85\n",
            "12. PHP                                                      83\n",
            "13. ISO_4217                                                 76\n",
            "14. Boston                                                   75\n",
            "15. Special:Search                                           74\n",
            "16. Wikimedia_Commons                                        70\n",
            "17. Pennsylvania                                             69\n",
            "18. Andreas_Schimpf                                          68\n",
            "19. Harriet_Gouldsmith                                       68\n",
            "20. Lorens_Frolich                                           68\n",
            "\n",
            "Execution time: 4.1353 seconds\n"
          ]
        }
      ],
      "source": [
        "def spark_loops_title_counts():\n",
        "    all_lines = valid_lines.collect()  \n",
        "\n",
        "    title_counts = {}\n",
        "\n",
        "    for line in all_lines:\n",
        "        title = line.split()[1]  \n",
        "        title_counts[title] = title_counts.get(title, 0) + 1  \n",
        "\n",
        "    # Sort titles by count in descending order\n",
        "    sorted_titles = sorted(title_counts.items(), key=lambda x: -x[1])\n",
        "\n",
        "    return {\n",
        "        'total_unique': len(sorted_titles),\n",
        "        'top_titles': sorted_titles[:20],  # Top 20 titles\n",
        "        'all_counts': sorted_titles        # All titles\n",
        "    }\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "title_stats_loop = spark_loops_title_counts()\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Total unique titles: {title_stats_loop['total_unique']:,}\")\n",
        "print(\"\\nTop 20 titles by count:\")\n",
        "for idx, (title, count) in enumerate(title_stats_loop['top_titles'], 1):\n",
        "    print(f\"{idx:2d}. {title[:50]:<50} {count:>8,}\")\n",
        "\n",
        "print(f\"\\nExecution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Comparison for Title Counts ===\n",
            "Map-Reduce Time: 17.2417 sec\n",
            "Python Loop Time: 6.3382 sec\n",
            "Difference: 10.9035 sec\n",
            "Python Loop was 2.7x faster\n",
            "\n",
            "Performance Comparison Results:\n",
            "+-----------+-------------+---------+------------------+\n",
            "|Approach   |Unique_Titles|Top_Count|Time_sec          |\n",
            "+-----------+-------------+---------+------------------+\n",
            "|Map-Reduce |2968690      |118      |17.241707400011364|\n",
            "|Python Loop|2968690      |118      |6.338167099980637 |\n",
            "+-----------+-------------+---------+------------------+\n",
            "\n",
            "\n",
            "=== Verification ===\n",
            "Unique counts match: True\n",
            "Top count match: True\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison for Title Counts \n",
        "print(\"\\n=== Performance Comparison for Title Counts ===\")\n",
        "print(f\"Map-Reduce Time: {mr_time:.4f} sec\")\n",
        "print(f\"Python Loop Time: {loop_time:.4f} sec\")\n",
        "print(f\"Difference: {abs(mr_time - loop_time):.4f} sec\")\n",
        "print(f\"Python Loop was {mr_time/loop_time:.1f}x faster\")\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Approach\", StringType(), True),\n",
        "    StructField(\"Unique_Titles\", LongType(), True),\n",
        "    StructField(\"Top_Count\", LongType(), True),\n",
        "    StructField(\"Time_sec\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "top_count_mr = title_stats['top_titles'][0][1] if title_stats['top_titles'] else 0\n",
        "top_count_loop = title_stats_loop['top_titles'][0][1] if title_stats_loop['top_titles'] else 0\n",
        "\n",
        "data = [\n",
        "    Row(\"Map-Reduce\", title_stats['total_unique'], top_count_mr, float(mr_time)),\n",
        "    Row(\"Python Loop\", title_stats_loop['total_unique'], top_count_loop, float(loop_time))\n",
        "]\n",
        "\n",
        "title_comparison_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(\"\\nPerformance Comparison Results:\")\n",
        "title_comparison_df.show(truncate=False)\n",
        "\n",
        "print(\"\\n=== Verification ===\")\n",
        "print(f\"Unique counts match: {title_stats['total_unique'] == title_stats_loop['total_unique']}\")\n",
        "print(f\"Top count match: {top_count_mr == top_count_loop}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ✅Observations:\n",
        "- The **Python Loop approach** outperforms the **Map-Reduce approach** in terms of execution time by a significant margin (3.0x faster).\n",
        "- Both approaches produce identical results for **Unique Titles** and **Top Count**, confirming that the logic and data are consistent across both methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Grouping by Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach: Group and Show Duplicate Pages by Title ===\n",
            "\n",
            "Sample output (first 5 titles with duplicates):\n",
            "\n",
            "Title: Indonesian_Wikipedia\n",
            "  - aa Indonesian_Wikipedia 1 4679\n",
            "  - en Indonesian_Wikipedia 1 93905\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  - aa Special:WhatLinksHere/Main_Page 1 5556\n",
            "  - commons.m Special:WhatLinksHere/Main_Page 2 15231\n",
            "  - en Special:WhatLinksHere/Main_Page 5 101406\n",
            "  - en.s Special:WhatLinksHere/Main_Page 1 8597\n",
            "  - en.voy Special:WhatLinksHere/Main_Page 1 8550\n",
            "  - meta.m Special:WhatLinksHere/Main_Page 1 11529\n",
            "  - outreach.m Special:WhatLinksHere/Main_Page 1 5698\n",
            "  - simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "\n",
            "Title: User_talk:Logan\n",
            "  - aa User_talk:Logan 1 4734\n",
            "  - en.voy User_talk:Logan 5 78175\n",
            "\n",
            "Title: Special:UserLogin\n",
            "  - aa.d Special:UserLogin 1 4899\n",
            "  - commons.m Special:UserLogin 30 181938\n",
            "  - en Special:UserLogin 44198 718770014\n",
            "  - en.q Special:UserLogin 4 34449\n",
            "  - incubator.m Special:UserLogin 1 5221\n",
            "  - m.f Special:UserLogin 13 58547\n",
            "  - m.w Special:UserLogin 3 12523\n",
            "  - m.wd Special:UserLogin 2 8696\n",
            "  - meta.m Special:UserLogin 1 5311\n",
            "  - simple Special:UserLogin 2 9960\n",
            "  - ss Special:UserLogin 1 5052\n",
            "  - www.w Special:UserLogin 5 49952\n",
            "  - www.wd Special:UserLogin 1 4989\n",
            "\n",
            "Title: User:CommonsDelinker\n",
            "  - aa.d User:CommonsDelinker 1 23375\n",
            "  - meta.m User:CommonsDelinker 1 9445\n",
            "\n",
            "\n",
            "Execution time: 8.6617 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach: Group and Show Duplicate Pages by Title ===\")\n",
        "\n",
        "def map_reduce_show_duplicates():\n",
        "    title_line_pairs = valid_lines.map(lambda line: (line.split()[1], line))\n",
        "    \n",
        "    grouped_by_title = title_line_pairs.groupByKey()\n",
        "    \n",
        "    duplicated_titles = grouped_by_title.filter(lambda x: len(list(x[1])) > 1)\n",
        "    \n",
        "    return duplicated_titles\n",
        "\n",
        "# Time execution\n",
        "start_time = timeit.default_timer()\n",
        "duplicates_rdd = map_reduce_show_duplicates()\n",
        "sample_duplicates = duplicates_rdd.take(5)\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(\"\\nSample output (first 5 titles with duplicates):\\n\")\n",
        "for title, pages in sample_duplicates:\n",
        "    print(f\"Title: {title}\")\n",
        "    for page in pages:\n",
        "        print(f\"  - {page}\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Simplified Map-Reduce for Duplicate Titles ===\n",
            "Title: Indonesian_Wikipedia\n",
            "  Total Hits: 2, Total Size: 98584\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  Total Hits: 15, Total Size: 188712\n",
            "\n",
            "Title: User_talk:Logan\n",
            "  Total Hits: 6, Total Size: 82909\n",
            "\n",
            "Title: Special:UserLogin\n",
            "  Total Hits: 44262, Total Size: 719151551\n",
            "\n",
            "Title: User:CommonsDelinker\n",
            "  Total Hits: 2, Total Size: 32820\n",
            "\n",
            "\n",
            "Execution time: 8.6617 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Simplified Map-Reduce for Duplicate Titles ===\")\n",
        "\n",
        "parsed_data = valid_lines.map(lambda line: (\n",
        "    (lambda parts: (parts[1], (int(parts[2]), int(parts[3]), line)))(line.split())\n",
        "))\n",
        "\n",
        "grouped_data = parsed_data.groupByKey()\n",
        "\n",
        "duplicate_results = grouped_data.flatMap(lambda x: [\n",
        "    (x[0], (\n",
        "        sum(h for h, s, l in x[1]),          # Total hits\n",
        "        sum(s for h, s, l in x[1]),          # Total size\n",
        "        [l for h, s, l in x[1]]              # Raw lines\n",
        "    )) \n",
        "] if len(list(x[1])) > 1 else [])\n",
        "\n",
        "# Display results\n",
        "sample = duplicate_results.take(5)\n",
        "for title, (total_hits, total_size, lines) in sample:\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"  Total Hits: {total_hits}, Total Size: {total_size}\")\n",
        "    #print(\"  Raw Lines:\")\n",
        "    #for line in lines:\n",
        "     #   print(f\"    - {line}\")\n",
        "    print()\n",
        "    \n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loop-Based Approach Matching Map-Reduce Output ===\n",
            "\n",
            "Sample output (first 5 duplicated titles with combined stats):\n",
            "\n",
            "Title: 271_a.C\n",
            "  Total Hits: 4\n",
            "  Total Size: 22386\n",
            "  Raw Lines:\n",
            "    - aa 271_a.C 1 4675\n",
            "    - az 271_a.C 1 6356\n",
            "    - bcl 271_a.C 1 5068\n",
            "    - be 271_a.C 1 6287\n",
            "\n",
            "Title: Category:User_th\n",
            "  Total Hits: 2\n",
            "  Total Size: 4770\n",
            "  Raw Lines:\n",
            "    - aa Category:User_th 1 4770\n",
            "    - commons.m Category:User_th 1 0\n",
            "\n",
            "Title: Chiron_Elias_Krase\n",
            "  Total Hits: 6\n",
            "  Total Size: 34584\n",
            "  Raw Lines:\n",
            "    - aa Chiron_Elias_Krase 1 4694\n",
            "    - az Chiron_Elias_Krase 1 6374\n",
            "    - bg Chiron_Elias_Krase 1 7468\n",
            "    - cho Chiron_Elias_Krase 1 4684\n",
            "    - dz Chiron_Elias_Krase 1 5435\n",
            "    - it Chiron_Elias_Krase 1 5929\n",
            "\n",
            "Title: Dassault_rafaele\n",
            "  Total Hits: 4\n",
            "  Total Size: 21940\n",
            "  Raw Lines:\n",
            "    - aa Dassault_rafaele 2 9372\n",
            "    - en Dassault_rafaele 1 6649\n",
            "    - it Dassault_rafaele 1 5919\n",
            "\n",
            "Title: E.Desv\n",
            "  Total Hits: 6\n",
            "  Total Size: 31539\n",
            "  Raw Lines:\n",
            "    - aa E.Desv 1 4662\n",
            "    - arc E.Desv 1 5210\n",
            "    - ast E.Desv 1 4825\n",
            "    - fiu-vro E.Desv 1 5237\n",
            "    - fr E.Desv 1 7057\n",
            "    - ik E.Desv 1 4548\n",
            "\n",
            "\n",
            "Execution time: 11.1926 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Loop-Based Approach Matching Map-Reduce Output ===\")\n",
        "\n",
        "def loop_based_approach():\n",
        "    collected_data = valid_lines.collect()\n",
        "    \n",
        "    title_dict = {}\n",
        "    \n",
        "    for line in collected_data:\n",
        "        parts = line.split()\n",
        "        title = parts[1]\n",
        "        hits = int(parts[2])\n",
        "        size = int(parts[3])\n",
        "        \n",
        "        if title not in title_dict:\n",
        "            title_dict[title] = {\n",
        "                'lines': [],\n",
        "                'total_hits': 0,\n",
        "                'total_size': 0,\n",
        "                'count': 0\n",
        "            }\n",
        "        \n",
        "        title_dict[title]['lines'].append(line)\n",
        "        title_dict[title]['total_hits'] += hits\n",
        "        title_dict[title]['total_size'] += size\n",
        "        title_dict[title]['count'] += 1\n",
        "    \n",
        "    duplicates = {k: v for k, v in title_dict.items() if v['count'] > 1}\n",
        "    \n",
        "    return duplicates\n",
        "\n",
        "# Time execution\n",
        "start_time = timeit.default_timer()\n",
        "duplicates = loop_based_approach()\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSample output (first 5 duplicated titles with combined stats):\\n\")\n",
        "for i, (title, details) in enumerate(duplicates.items()):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"  Total Hits: {details['total_hits']}\")\n",
        "    print(f\"  Total Size: {details['total_size']}\")\n",
        "    print(\"  Raw Lines:\")\n",
        "    for line in details['lines']:\n",
        "        print(f\"    - {line}\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nExecution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ✅ Observations\n",
        "\n",
        "- **Map-Reduce Approach**: 11.21 seconds\n",
        "- **Loop-Based Approach**: 20.10 seconds\n",
        "\n",
        "The **Map-Reduce approach** is significantly faster due to its parallel processing capabilities, which allows tasks to be distributed across multiple nodes, reducing execution time. In contrast, the **Loop-Based approach** processes data sequentially, which becomes slower as the dataset grows, leading to longer execution times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Map-Reduce Approach for Combined Page Pairs ===\n",
            "=== Sample Combined Page Pairs ===\n",
            "\n",
            "Title: Indonesian_Wikipedia\n",
            "  Pair: aa Indonesian_Wikipedia 1 4679  <==>  en Indonesian_Wikipedia 1 93905\n",
            "\n",
            "Title: Special:WhatLinksHere/Main_Page\n",
            "  Pair: aa Special:WhatLinksHere/Main_Page 1 5556  <==>  commons.m Special:WhatLinksHere/Main_Page 2 15231\n",
            "  Pair: aa Special:WhatLinksHere/Main_Page 1 5556  <==>  en Special:WhatLinksHere/Main_Page 5 101406\n",
            "  Pair: aa Special:WhatLinksHere/Main_Page 1 5556  <==>  en.s Special:WhatLinksHere/Main_Page 1 8597\n",
            "  Pair: aa Special:WhatLinksHere/Main_Page 1 5556  <==>  en.voy Special:WhatLinksHere/Main_Page 1 8550\n",
            "  Pair: aa Special:WhatLinksHere/Main_Page 1 5556  <==>  meta.m Special:WhatLinksHere/Main_Page 1 11529\n",
            "  Pair: aa Special:WhatLinksHere/Main_Page 1 5556  <==>  outreach.m Special:WhatLinksHere/Main_Page 1 5698\n",
            "  Pair: aa Special:WhatLinksHere/Main_Page 1 5556  <==>  simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "  Pair: commons.m Special:WhatLinksHere/Main_Page 2 15231  <==>  en Special:WhatLinksHere/Main_Page 5 101406\n",
            "  Pair: commons.m Special:WhatLinksHere/Main_Page 2 15231  <==>  en.s Special:WhatLinksHere/Main_Page 1 8597\n",
            "  Pair: commons.m Special:WhatLinksHere/Main_Page 2 15231  <==>  en.voy Special:WhatLinksHere/Main_Page 1 8550\n",
            "  Pair: commons.m Special:WhatLinksHere/Main_Page 2 15231  <==>  meta.m Special:WhatLinksHere/Main_Page 1 11529\n",
            "  Pair: commons.m Special:WhatLinksHere/Main_Page 2 15231  <==>  outreach.m Special:WhatLinksHere/Main_Page 1 5698\n",
            "  Pair: commons.m Special:WhatLinksHere/Main_Page 2 15231  <==>  simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "  Pair: en Special:WhatLinksHere/Main_Page 5 101406  <==>  en.s Special:WhatLinksHere/Main_Page 1 8597\n",
            "  Pair: en Special:WhatLinksHere/Main_Page 5 101406  <==>  en.voy Special:WhatLinksHere/Main_Page 1 8550\n",
            "  Pair: en Special:WhatLinksHere/Main_Page 5 101406  <==>  meta.m Special:WhatLinksHere/Main_Page 1 11529\n",
            "  Pair: en Special:WhatLinksHere/Main_Page 5 101406  <==>  outreach.m Special:WhatLinksHere/Main_Page 1 5698\n",
            "  Pair: en Special:WhatLinksHere/Main_Page 5 101406  <==>  simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "  Pair: en.s Special:WhatLinksHere/Main_Page 1 8597  <==>  en.voy Special:WhatLinksHere/Main_Page 1 8550\n",
            "  Pair: en.s Special:WhatLinksHere/Main_Page 1 8597  <==>  meta.m Special:WhatLinksHere/Main_Page 1 11529\n",
            "  Pair: en.s Special:WhatLinksHere/Main_Page 1 8597  <==>  outreach.m Special:WhatLinksHere/Main_Page 1 5698\n",
            "  Pair: en.s Special:WhatLinksHere/Main_Page 1 8597  <==>  simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "  Pair: en.voy Special:WhatLinksHere/Main_Page 1 8550  <==>  meta.m Special:WhatLinksHere/Main_Page 1 11529\n",
            "  Pair: en.voy Special:WhatLinksHere/Main_Page 1 8550  <==>  outreach.m Special:WhatLinksHere/Main_Page 1 5698\n",
            "  Pair: en.voy Special:WhatLinksHere/Main_Page 1 8550  <==>  simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "  Pair: meta.m Special:WhatLinksHere/Main_Page 1 11529  <==>  outreach.m Special:WhatLinksHere/Main_Page 1 5698\n",
            "  Pair: meta.m Special:WhatLinksHere/Main_Page 1 11529  <==>  simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "  Pair: outreach.m Special:WhatLinksHere/Main_Page 1 5698  <==>  simple Special:WhatLinksHere/Main_Page 3 32145\n",
            "\n",
            "Title: User_talk:Logan\n",
            "  Pair: aa User_talk:Logan 1 4734  <==>  en.voy User_talk:Logan 5 78175\n",
            "MapReduce Pairwise Combination Time: 11.08 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Map-Reduce Approach for Combined Page Pairs ===\")\n",
        "\n",
        "duplicate_titles = duplicates_rdd.keys().collect()  \n",
        "duplicate_titles_set = set(duplicate_titles)\n",
        "\n",
        "title_line_pairs = valid_lines.map(lambda line: (line.split()[1], line))\n",
        "grouped_by_title = title_line_pairs.groupByKey().mapValues(list)\n",
        "\n",
        "filtered_grouped = grouped_by_title.filter(lambda x: x[0] in duplicate_titles_set)\n",
        "\n",
        "def create_combinations(title_entries):\n",
        "    title, entries = title_entries\n",
        "    pairs = list(combinations(entries, 2))  # all unique 2-combinations\n",
        "    return (title, pairs) if pairs else None\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "combined = filtered_grouped.map(create_combinations).filter(lambda x: x is not None)\n",
        "\n",
        "sample = combined.take(3)\n",
        "\n",
        "print(\"=== Sample Combined Page Pairs ===\")\n",
        "for title, pairs in sample:\n",
        "    print(f\"\\nTitle: {title}\")\n",
        "    for pair in pairs:\n",
        "        print(f\"  Pair: {pair[0]}  <==>  {pair[1]}\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"MapReduce Pairwise Combination Time: {end - start:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All results written to combined_same_title_page_pairs.txt\n"
          ]
        }
      ],
      "source": [
        "all_combined = combined.collect()\n",
        "\n",
        "# Write all results to a file\n",
        "output_path = \"combined_same_title_page_pairs.txt\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== Combined Page Pairs for Duplicate Titles ===\\n\")\n",
        "    for title, pairs in all_combined:\n",
        "        f.write(f\"\\nTitle: {title}\\n\")\n",
        "        for pair in pairs:\n",
        "            f.write(f\"  Pair: {pair[0]}  <==>  {pair[1]}\\n\")\n",
        "\n",
        "print(f\"All results written to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Title Pairwise Combinations (Based on Aggregation) ===\n",
            "=== Sample Aggregated Page Pairs ===\n",
            "  Pair: Indonesian_Wikipedia 2 98584  <==>  Special:MyLanguage/Meta:Index 1 4701\n",
            "  Pair: Indonesian_Wikipedia 2 98584  <==>  Special:WhatLinksHere/Main_Page 15 188712\n",
            "  Pair: Indonesian_Wikipedia 2 98584  <==>  Special:WhatLinksHere/MediaWiki:Edittools 1 5139\n",
            "  Pair: Indonesian_Wikipedia 2 98584  <==>  User:IlStudioso 1 6796\n",
            "  Pair: Indonesian_Wikipedia 2 98584  <==>  User_talk:Logan 6 82909\n",
            "\n",
            "Combination Generation Time: 10.20 seconds\n",
            "Combinations written to aggregated_title_pairs.out\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Title Pairwise Combinations (Based on Aggregation) ===\")\n",
        "aggregated = valid_lines.map(lambda line: (\n",
        "    line.split()[1],  # title\n",
        "    (int(line.split()[2]), int(line.split()[3]))  # (hits, size)\n",
        ")).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "\n",
        "aggregated_lines = aggregated.map(lambda x: f\"{x[0]} {x[1][0]} {x[1][1]}\")\n",
        "start = time.time()\n",
        "\n",
        "aggregated_sample = aggregated_lines.take(10)\n",
        "pairs = list(combinations(aggregated_sample, 2))  \n",
        "\n",
        "# Step 5: Write combinations to file\n",
        "pairs_output_path = \"aggregated_title_pairs.out\"\n",
        "with open(pairs_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== Pairwise Combinations of Aggregated Titles ===\\n\")\n",
        "    for pair in pairs:\n",
        "        f.write(f\"Pair: {pair[0]}  <==>  {pair[1]}\\n\")\n",
        "\n",
        "# Step 6: Display only first few pairs\n",
        "print(\"=== Sample Aggregated Page Pairs ===\")\n",
        "for pair in pairs[:5]:\n",
        "    print(f\"  Pair: {pair[0]}  <==>  {pair[1]}\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"\\nCombination Generation Time: {end - start:.2f} seconds\")\n",
        "print(f\"Combinations written to {pairs_output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Unique Title Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Map-Reduce Approach: Determine the number of unique terms appearing in the page titles ===\n",
            "Total number of unique terms: 850479\n",
            "\n",
            "Execution time: 12.6292 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Map-Reduce Approach: Determine the number of unique terms appearing in the page titles ===\")\n",
        "\n",
        "def map_reduce_unqiue_title_terms():\n",
        "    terms = valid_lines.flatMap(lambda line: line.split()[1].split(\"_\"))\n",
        "    normalized_terms = terms.map(lambda term: term.lower())\\\n",
        "                            .filter(lambda term: term.isalnum())\\\n",
        "                            .filter(lambda term: term not in stop_words)\n",
        "    unique_terms = normalized_terms.distinct()\n",
        "    return unique_terms\n",
        "\n",
        "# Time the execution\n",
        "start_time = timeit.default_timer()\n",
        "unique_terms = map_reduce_unqiue_title_terms()\n",
        "unique_terms_count = unique_terms.count()\n",
        "mr_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(f\"Total number of unique terms: {unique_terms_count}\")\n",
        "print(f\"\\nExecution time: {mr_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loop-Based Approach Matching Map-Reduce Output ===\n",
            "Total number of unique terms: 850480\n",
            "\n",
            "Execution time: 3.6609 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Loop-Based Approach Matching Map-Reduce Output ===\")\n",
        "\n",
        "def spark_loops_unqiue_title_terms():\n",
        "    unique_terms = set()\n",
        "    with open(data_path, 'r') as f:\n",
        "        for line in f:\n",
        "            terms = line.split()[1].split(\"_\")\n",
        "            for term in terms:\n",
        "                term = term.lower()\n",
        "                if term.isalnum() and term not in stop_words:\n",
        "                    unique_terms.add(term)\n",
        "    return unique_terms\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "unique_terms = spark_loops_unqiue_title_terms()\n",
        "unique_terms_count = len(unique_terms)\n",
        "loop_time = timeit.default_timer() - start_time\n",
        "\n",
        "print(f\"Total number of unique terms: {unique_terms_count}\")\n",
        "print(f\"\\nExecution time: {loop_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Performance Comparison ===\n",
            "Map-Reduce Time: 12.6292 sec\n",
            "Spark Loops Time: 3.6609 sec\n",
            "Difference: 8.9683 sec\n",
            "Faster by: 3.45x\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison \n",
        "print(\"\\n=== Performance Comparison ===\")\n",
        "print(f\"Map-Reduce Time: {mr_time:.4f} sec\")\n",
        "print(f\"Spark Loops Time: {loop_time:.4f} sec\")\n",
        "print(f\"Difference: {abs(mr_time - loop_time):.4f} sec\")\n",
        "print(f\"Faster by: {max(mr_time, loop_time)/min(mr_time, loop_time):.2f}x\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
